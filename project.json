{
  "project_name": "Poor AI Programming Tool",
  "files": [
    {
      "name": "core/file_handler.py",
      "short": "Advanced file handler class with project JSON support, file loading, saving, and smart updating.",
      "detailed": "This module provides the FileHandler class, which manages loading and saving files, updating and maintaining a project.json metadata file, supporting wildcards, file diffs, and the preservation of language-specific comments. Logging and careful error handling are included throughout.",
      "language": "python"
    },
    {
      "name": "core/model_manager.py",
      "short": "Manages AI models, API calls, and displays request/response interactions.",
      "detailed": "Contains the ModelManager class for loading model/app configurations, switching models, and orchestrating prompt-response cycles across multiple providers (e.g., ollama, openrouter, fake). Handles API errors robustly, logs activity, and creates human-readable request-response logs for debugging or transparency.",
      "language": "python"
    },
    {
      "name": "core/prompt_processor.py",
      "short": "Processes user commands into structured prompts for the AI model.",
      "detailed": "Refactored for clarity and maintainability. Improved comments, type annotations, and logical flow. _parse_command and _build_context now have explicitly documented behavior. No functional change, but improved code readability and extension points.",
      "language": "python"
    },
    {
      "name": "core/response_processor.py",
      "short": "Extractor for code and diffs from LLM or API responses, with JSON and various text/diff format handling.",
      "detailed": "This file provides an extract_code function to parse file-content pairs from raw string responses that may be JSON arrays, code fences, diffs, or just textual blocks, handling many edge-cases for robust integration with code-gen and documentation LLM workflows. It escapes triple backticks in output and annotates artifacts with automatic UUIDs and format metadata. The module supports parsing standard code blocks and both unified and git diff formats, suitable for programmatic extraction of artifacts from LLM-driven outputs.",
      "language": "python"
    },
    {
      "name": "core/template_processor.py",
      "short": "Template processing utility for injecting project and file information into templates, with support for descriptions and file handlers.",
      "detailed": "Fixes the issue in _get_json_file_contents so that the 'filename' field, when replacing a placeholder, will contain the relative path (including directory) instead of just the basename. The rest of the code and logic remain unchanged and backward-compatible.",
      "language": "python"
    },
    {
      "name": "poor_ai.py",
      "short": "Main CLI for Poor AI tool, handles user commands, manages project state, integrates with core components.",
      "detailed": "This Python script implements the CLI application for the Poor AI Programming Tool, providing the main interactive command loop for users. It supports project folder structure validation, loading and saving files, AI generation, template and description management, model selection, and more. The script integrates with separate core modules (FileHandler, PromptProcessor, ModelManager, TemplateProcessor, extract_code). Extensive logging is set up (with log file rotation), and the application gracefully handles IO and user errors. When the user runs the tool, it parses commands, interacts with the core logic, and manages state for the current files, model, template, and task prompt. The code is modular, using methods for all major actions, and provides helpful command-line feedback.",
      "language": "python"
    },
    {
      "name": "test.txt",
      "short": null,
      "detailed": null,
      "language": null
    }
  ]
}