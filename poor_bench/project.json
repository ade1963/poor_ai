{
  "project_name": "Poor Bench",
  "files": [
    {
      "name": "DevGuide_v1.2.md",
      "short": null,
      "detailed": null,
      "language": null
    },
    {
      "name": "llm_manager.py",
      "short": "Central module for all LLM API calls and prompt dispatching.",
      "detailed": "Implements the LLMManager class with per-provider internal helpers (OpenAI and Ollama) to send prompts for LLM inference. Handles system/work prompt merging, tokenizes LLM IDs, applies LLM-specific parameter logic, manages timing, error catching, and yields simple unified responses and timing results. All network API calls are implemented here; other project components should import and use this module for LLM invocation.",
      "language": "python"
    },
    {
      "name": "poor_bench/__init__.py",
      "short": "Initialization file for the poor_bench package.",
      "detailed": "Makes the poor_bench directory a Python package. This file can be empty or can be used to expose key components of the package at the top level.",
      "language": "python"
    },
    {
      "name": "poor_bench/config_manager.py",
      "short": "Manages loading and saving of configuration files and results.",
      "detailed": "Provides methods to load test classes (YAML), test instances (JSON), LLM configurations (JSON), and test results (JSON). It also includes functionality to save results and determine pending tests for a given LLM.",
      "language": "python"
    },
    {
      "name": "poor_bench/evaluators/__init__.py",
      "short": "Initialization file for the evaluators sub-package.",
      "detailed": "Makes the evaluators directory a Python sub-package. This file is typically empty.",
      "language": "python"
    },
    {
      "name": "poor_bench/evaluators/math_evaluator.py",
      "short": "Evaluation module for math problems.",
      "detailed": "Handles responses for math problems, typically expecting JSON output. It compares the LLM's answer to an expected answer with a specified precision.",
      "language": "python"
    },
    {
      "name": "poor_bench/evaluators/python_evaluator.py",
      "short": "Evaluation module for Python coding tasks.",
      "detailed": "Evaluates Python code generated by an LLM. It extracts code, checks syntax, executes it against a set of test cases, and compares outputs. Supports function name checking and execution timeout.",
      "language": "python"
    },
    {
      "name": "poor_bench/evaluators/sentiment_evaluator.py",
      "short": "Evaluation module for sentiment analysis tasks.",
      "detailed": "Evaluates LLM responses for sentiment analysis, expecting a JSON list of sentiments. It compares the identified sentiments against a list of expected sentiments.",
      "language": "python"
    },
    {
      "name": "poor_bench/llms.json",
      "short": "Configuration file defining Large Language Models (LLMs) and their parameters.",
      "detailed": "Contains a list of LLM objects, each specifying the provider, model name, API endpoint, optional base system prompt, API key environment variable (if needed), and other LLM-specific parameters like temperature or max tokens.",
      "language": "json"
    },
    {
      "name": "poor_bench/main.py",
      "short": "Main command-line interface for the Poor Bench framework.",
      "detailed": "Provides CLI commands to run tests, generate reports, and list configurations (LLMs, tests, test classes). Uses argparse for command parsing and orchestrates calls to ConfigManager, LLMManager, TestRunner, and ReportGenerator.",
      "language": "python"
    },
    {
      "name": "poor_bench/report_generator.py",
      "short": "Generates reports from test results in various formats.",
      "detailed": "The ReportGenerator class loads test results using ConfigManager and can produce reports like a console summary, CSV file, or JSON file. Reports can be filtered by LLM IDs or test IDs.",
      "language": "python"
    },
    {
      "name": "poor_bench/results.json",
      "short": "File for storing test results, organized by LLM ID.",
      "detailed": "Logs results from test runs. It includes a version and a dictionary where keys are LLM identifiers and values are lists of result objects. Each result object contains test ID, score, details, raw LLM response, timestamp, and execution time.",
      "language": "json"
    },
    {
      "name": "poor_bench/test_classes.yaml",
      "short": "Configuration file defining test classes, prompt templates, and evaluation modules.",
      "detailed": "Contains definitions for different categories of tests, including their descriptions, system prompts, work prompt templates (with LLM-specific overrides), and default evaluation module configurations.",
      "language": "yaml"
    },
    {
      "name": "poor_bench/test_runner.py",
      "short": "Coordinates the execution of individual tests against specified LLMs.",
      "detailed": "The TestRunner class is responsible for running a single test: it loads test data, LLM configuration, and test class details; constructs the appropriate prompt; uses LLMManager to get the LLM's response; dynamically loads and calls an evaluator module; and saves the result via ConfigManager.",
      "language": "python"
    },
    {
      "name": "poor_bench/tests.json",
      "short": "Configuration file storing specific test instances linked to test classes.",
      "detailed": "Contains an array of test objects, each with a unique ID, a link to a test class, difficulty level, test content (text or texts), and specific evaluation module parameters for that instance.",
      "language": "json"
    }
  ]
}