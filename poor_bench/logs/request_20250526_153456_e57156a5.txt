Model: google/gemini-2.5-pro-preview
Provider: openrouter
Prompt Tokens: 2582
Completion Tokens: 4548
Total Cost: $0.048708
System Prompt: Please write the filename on the first line (e.g. main.py), then the code in triple backticks on the next lines. For markdown content that includes triple backticks, use four backticks (````) to wrap the content. Do NOT include language hints or explanations.
Temperature: 1.0
Top P: 0.95
Max Tokens: 256000
Timestamp: 20250526_153456

Request:
**[PROJECT NAME]**
Poor Bench

**[TASK]**
Implement remaining modules of the project: main.py, config_manager.py, etc., and all config files. Be brave.

**[PROJECT FILE LIST]**
- DevGuide_v1.1.md
- llm_manager.py
  Short: Central module for all LLM API calls and prompt dispatching.

**[FILE CONTENTS]**
```json
[
  {
    "filename": "DevGuide_v1.1.md",
    "language": "markdown",
    "content": "# Developer Guide for `poor_bench` v1.1\n\n## Overview\n\n`poor_bench` is a Python-based benchmarking framework designed to create, manage, and evaluate tests for Large Language Models (LLMs). It empowers developers to define test classes with customizable prompt templates, generate tests across multiple difficulty levels, and evaluate results using modular evaluation components. The tool supports tailored prompts per LLM, initial test validation, and bulk test execution, with results scored on a 0.0 to 1.0 scale. As a lightweight, console-based solution optimized for constrained systems, it operates as a standalone module within the `poor_ai` ecosystem.\n\nA key focus of `poor_bench` is to catch potential LLM issues early in development, enabling fine-tuning of parameters and system prompts to boost performance and reliability.\n\n### Project Structure\n`poor_bench` resides as a subdirectory within `poor_ai`:\n\n```\npoor_ai/\n\u251c\u2500\u2500 poor_bench/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u251c\u2500\u2500 config_manager.py\n\u2502   \u251c\u2500\u2500 test_runner.py\n\u2502   \u251c\u2500\u2500 report_generator.py\n\u2502   \u251c\u2500\u2500 evaluators/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 math_evaluator.py\n\u2502   \u2502   \u251c\u2500\u2500 sentiment_evaluator.py\n\u2502   \u2502   \u2514\u2500\u2500 python_evaluator.py\n\u2502   \u251c\u2500\u2500 test_classes.yaml\n\u2502   \u251c\u2500\u2500 tests.json\n\u2502   \u251c\u2500\u2500 llms.json\n\u2502   \u2514\u2500\u2500 results.json\n\u2514\u2500\u2500 (other poor_ai files)\n```\n\n### Development Environment Setup\n\n1. **Python Version**: Use Python 3.9+ for development\n2. **Dependencies**:\n   ```\n   pip install pyyaml matplotlib numpy pandas pytest requests\n   ```\n3. **Development Tools**:\n   - Use `pytest` for unit testing\n   - Use `black` for code formatting\n   - Use `pylint` for linting\n\n## Data Structures\n\n`poor_bench` uses YAML for test class configurations and JSON for test instances, LLM configurations, and results. Each file includes a version field for tracking changes.\n\n### 1. Test Class Configuration (`test_classes.yaml`)\n\nDefines test classes with prompt templates and evaluation modules.\n\n#### Schema\n- `version`: String, schema version (e.g., \"1.0\").\n- `test_classes`: List of test class objects.\n  - `id`: Unique string identifier (e.g., \"math_problems\").\n  - `description`: Human-readable description.\n  - `system_prompt`: Optional string for LLM system-level instruction.\n  - `work_prompt_template`:\n    - `default`: String with placeholders (e.g., `{problem}`).\n    - `overrides`: Optional dictionary mapping LLM IDs to custom prompts.\n  - `evaluation_module`:\n    - `name`: String, evaluation module name (e.g., \"math_evaluator\").\n    - `parameters`: Key-value pairs for module settings.\n\n#### Example\n```yaml\nversion: \"1.0\"\ntest_classes:\n  - id: \"math_problems\"\n    description: \"Tests for solving mathematical problems\"\n    system_prompt: \"You are a math expert. Provide clear solutions.\"\n    work_prompt_template:\n      default: \"Solve this math problem and return your answer in JSON format with key 'answer': {problem}\"\n      overrides:\n        ollama:gemma3:1b-it-qat: \"Solve this and return JSON with 'answer': {problem}\"\n    evaluation_module:\n      name: \"math_evaluator\"\n      parameters:\n        precision: 2\n        expected_format: \"json\"\n\n  - id: \"sentiment_analysis\"\n    description: \"Tests for determining sentiment of texts\"\n    system_prompt: \"You are a sentiment analysis expert.\"\n    work_prompt_template:\n      default: \"Analyze the sentiment of these texts and return a JSON list of sentiments ['positive', 'negative', 'neutral']: {texts}\"\n    evaluation_module:\n      name: \"sentiment_evaluator\"\n      parameters:\n        response_format: \"list_of_sentiments\"\n\n  - id: \"python_coding\"\n    description: \"Tests for writing Python code to solve programming problems\"\n    system_prompt: \"You are an expert Python programmer. Write clean, efficient, and well-documented code.\"\n    work_prompt_template:\n      default: \"Write a Python function that {task}. Include docstrings and comments where appropriate.\"\n    evaluation_module:\n      name: \"python_evaluator\"\n      parameters:\n        test_cases: []\n```\n\n### 2. Test Instances (`tests.json`)\n\nStores specific test instances linked to test classes.\n\n#### Schema\n- `version`: String, schema version (e.g., \"1.0\").\n- `tests`: List of test objects.\n  - `test_id`: Unique string identifier.\n  - `class_id`: Links to a test class `id`.\n  - `level`: Integer, difficulty level.\n  - `text` or `texts`: String or list, test content.\n  - `evaluation_module`:\n    - `name`: Evaluation module name.\n    - `parameters`: Evaluation settings.\n\n#### Example\n```json\n{\n  \"version\": \"1.0\",\n  \"tests\": [\n    {\n      \"test_id\": \"math_problems_level1_001\",\n      \"class_id\": \"math_problems\",\n      \"level\": 1,\n      \"text\": \"What is 2 + 2?\",\n      \"evaluation_module\": {\n        \"name\": \"math_evaluator\",\n        \"parameters\": {\n          \"expected_answer\": \"4\",\n          \"precision\": 0,\n          \"expected_format\": \"json\"\n        }\n      }\n    },\n    {\n      \"test_id\": \"sentiment_analysis_level1_001\",\n      \"class_id\": \"sentiment_analysis\",\n      \"level\": 1,\n      \"texts\": [\n        \"I love this product!\",\n        \"This is the worst experience ever.\",\n        \"It's okay, nothing special.\"\n      ],\n      \"evaluation_module\": {\n        \"name\": \"sentiment_evaluator\",\n        \"parameters\": {\n          \"expected_sentiments\": [\"positive\", \"negative\", \"neutral\"],\n          \"response_format\": \"list_of_sentiments\"\n        }\n      }\n    },\n    {\n      \"test_id\": \"python_coding_level2_001\",\n      \"class_id\": \"python_coding\",\n      \"level\": 2,\n      \"text\": \"calculates the factorial of a number\",\n      \"evaluation_module\": {\n        \"name\": \"python_evaluator\",\n        \"parameters\": {\n          \"test_cases\": [\n            {\"input\": 5, \"expected_output\": 120},\n            {\"input\": 0, \"expected_output\": 1},\n            {\"input\": 1, \"expected_output\": 1}\n          ],\n          \"function_name\": \"factorial\",\n          \"execution_timeout\": 2\n        }\n      }\n    }\n  ]\n}\n```\n\n### 3. LLM Configurations (`llms.json`)\n\nDefines LLMs and their parameters.\n\n#### Schema\n- `llms`: List of LLM objects.\n  - `provider`: String (e.g., \"ollama\").\n  - `name`: String, model name.\n  - `endpoint`: String, API endpoint URL.\n  - `base_system_prompt`: Optional default system prompt.\n  - `parameters`: Key-value pairs (e.g., `temperature`).\n\n#### Example\n```json\n{\n  \"llms\": [\n    {\n      \"provider\": \"ollama\",\n      \"name\": \"gemma3:1b-it-qat\",\n      \"endpoint\": \"http://127.0.0.1:11434/v1\",\n      \"base_system_prompt\": \"You are a helpful assistant.\",\n      \"parameters\": {\n        \"temperature\": 0.7,\n        \"top_p\": 0.9\n      }\n    },\n    {\n      \"provider\": \"openai\",\n      \"name\": \"gpt-4o\",\n      \"endpoint\": \"https://api.openai.com/v1\",\n      \"api_key_env\": \"OPENAI_API_KEY\",\n      \"parameters\": {\n        \"temperature\": 0.5,\n        \"max_tokens\": 1024\n      }\n    }\n  ]\n}\n```\n\n### 4. Test Results (`results.json`)\n\nLogs results, organized by LLM ID for easy querying and persistence.\n\n#### Schema\n- `version`: String, schema version (e.g., \"1.0\").\n- `results`: Dictionary with LLM IDs as keys and lists of result objects as values.\n  - `test_id`: Unique test identifier.\n  - `score`: Float, 0.0 to 1.0.\n  - `details`: Evaluation feedback.\n  - `response`: Raw LLM response.\n  - `timestamp`: ISO format timestamp.\n  - `execution_time_ms`: Response time in milliseconds.\n\n#### Example\n```json\n{\n  \"version\": \"1.0\",\n  \"results\": {\n    \"ollama:gemma3:1b-it-qat\": [\n      {\n        \"test_id\": \"math_problems_level1_001\",\n        \"score\": 1.0,\n        \"details\": \"Correctly answered '4'\",\n        \"response\": \"{\\\"answer\\\": 4}\",\n        \"timestamp\": \"2025-05-25T14:32:15.123456\",\n        \"execution_time_ms\": 245\n      }\n    ],\n    \"openai:gpt-4o\": [\n      {\n        \"test_id\": \"sentiment_analysis_level1_001\",\n        \"score\": 1.0,\n        \"details\": \"Correctly identified all sentiments\",\n        \"response\": \"[\\\"positive\\\", \\\"negative\\\", \\\"neutral\\\"]\",\n        \"timestamp\": \"2025-05-25T14:32:18.654321\",\n        \"execution_time_ms\": 189\n      }\n    ]\n  }\n}\n```\n\n## Evaluation Modules\n\nEvaluation modules score LLM outputs on a 0.0 to 1.0 scale.\n\n### Interface Specification\n```python\ndef evaluate(response: str, parameters: dict) -> tuple[float, str]:\n    \"\"\"\n    Evaluates an LLM response.\n    Args:\n        response: LLM output.\n        parameters: Evaluation settings.\n    Returns:\n        Tuple[float, str]: Score and details.\n    \"\"\"\n```\n\n### Example Evaluation Modules\n\n#### 1. `math_evaluator.py`\nHandles JSON responses for math problems.\n\n```python\nimport json\nfrom decimal import Decimal, InvalidOperation\n\ndef evaluate(response: str, parameters: dict) -> tuple[float, str]:\n    expected = parameters.get(\"expected_answer\")\n    precision = parameters.get(\"precision\", 2)\n    expected_format = parameters.get(\"expected_format\", \"json\")\n    \n    if expected_format == \"json\":\n        try:\n            response_data = json.loads(response)\n            actual = response_data.get(\"answer\")\n            if actual is None:\n                return 0.0, \"No 'answer' key in JSON response\"\n        except json.JSONDecodeError:\n            return 0.0, \"Invalid JSON in response\"\n    else:\n        return 0.0, \"Expected JSON format\"\n    \n    try:\n        expected_decimal = Decimal(str(expected))\n        actual_decimal = Decimal(str(actual))\n        if round(actual_decimal, precision) == round(expected_decimal, precision):\n            return 1.0, f\"Correct answer: {actual}\"\n        else:\n            return 0.0, f\"Expected {expected}, got {actual}\"\n    except InvalidOperation:\n        return 0.0, \"Invalid numeric format\"\n```\n\n#### 2. `sentiment_evaluator.py`\nEvaluates multiple sentiments from a JSON list.\n\n```python\nimport json\n\ndef evaluate(response: str, parameters: dict) -> tuple[float, str]:\n    expected_sentiments = parameters.get(\"expected_sentiments\", [])\n    response_format = parameters.get(\"response_format\", \"list_of_sentiments\")\n    \n    if response_format == \"list_of_sentiments\":\n        try:\n            actual_sentiments = json.loads(response)\n            if not isinstance(actual_sentiments, list):\n                return 0.0, \"Response must be a list\"\n        except json.JSONDecodeError:\n            return 0.0, \"Invalid JSON in response\"\n        \n        if len(actual_sentiments) != len(expected_sentiments):\n            return 0.0, f\"Expected {len(expected_sentiments)} sentiments, got {len(actual_sentiments)}\"\n        \n        correct = sum(1 for a, e in zip(actual_sentiments, expected_sentiments) if a.lower() == e.lower())\n        score = correct / len(expected_sentiments)\n        return score, f\"Correctly identified {correct} out of {len(expected_sentiments)} sentiments\"\n    else:\n        return 0.0, \"Expected list_of_sentiments format\"\n```\n\n#### 3. `python_evaluator.py`\n```python\nimport sys\nimport io\nimport traceback\nimport ast\nimport time\nfrom contextlib import redirect_stdout, redirect_stderr\nfrom typing import Dict, List, Any, Tuple\n\ndef evaluate(response: str, parameters: dict) -> tuple[float, str]:\n    \"\"\"\n    Evaluates Python code solutions.\n    \n    Args:\n        response: String, LLM's Python code solution.\n        parameters: Dict with keys:\n            - test_cases: List of dicts with 'input' and 'expected_output' keys\n            - function_name: Expected name of the function (optional)\n            - execution_timeout: Maximum execution time in seconds (default: 5)\n    \n    Returns:\n        Tuple[float, str]: Score (0.0 to 1.0) and explanation.\n    \"\"\"\n    # Extract code from the response\n    code_blocks = extract_code_blocks(response)\n    if not code_blocks:\n        return 0.0, \"No Python code found in the response\"\n    \n    # Use the largest code block (assuming it's the complete solution)\n    code = max(code_blocks, key=len)\n    \n    # Check if the code is syntactically valid\n    try:\n        ast.parse(code)\n    except SyntaxError as e:\n        return 0.0, f\"Syntax error in code: {str(e)}\"\n    \n    # Extract test cases and other parameters\n    test_cases = parameters.get(\"test_cases\", [])\n    function_name = parameters.get(\"function_name\", None)\n    timeout = parameters.get(\"execution_timeout\", 5)\n    \n    if not test_cases:\n        return 0.5, \"No test cases provided, code syntax is valid but functionality not verified\"\n    \n    # Create a safe execution environment\n    namespace = {}\n    \n    # Execute the code in the namespace\n    try:\n        exec(code, namespace)\n    except Exception as e:\n        return 0.0, f\"Error executing code: {str(e)}\"\n    \n    # Check if the expected function exists\n    if function_name and function_name not in namespace:\n        return 0.0, f\"Function '{function_name}' not found in code\"\n    \n    # If function_name is not specified, try to find a function in the namespace\n    if not function_name:\n        functions = [name for name, obj in namespace.items() \n                    if callable(obj) and not name.startswith('__')]\n        if not functions:\n            return 0.0, \"No functions defined in code\"\n        function_name = functions[0]\n    \n    # Run test cases\n    function = namespace[function_name]\n    passed_tests = 0\n    test_results = []\n    \n    for i, test_case in enumerate(test_cases):\n        input_value = test_case.get(\"input\")\n        expected_output = test_case.get(\"expected_output\")\n        \n        # Capture stdout and stderr\n        stdout_buffer = io.StringIO()\n        stderr_buffer = io.StringIO()\n        \n        try:\n            # Set timeout for execution\n            start_time = time.time()\n            \n            with redirect_stdout(stdout_buffer), redirect_stderr(stderr_buffer):\n                if isinstance(input_value, list):\n                    actual_output = function(*input_value)\n                elif isinstance(input_value, dict):\n                    actual_output = function(**input_value)\n                else:\n                    actual_output = function(input_value)\n            \n            execution_time = time.time() - start_time\n            if execution_time > timeout:\n                test_results.append(f\"Test {i+1}: TIMEOUT after {execution_time:.2f}s\")\n                continue\n                \n            # Compare output\n            if actual_output == expected_output:\n                passed_tests += 1\n                test_results.append(f\"Test {i+1}: PASS\")\n            else:\n                test_results.append(f\"Test {i+1}: FAIL - Expected {expected_output}, got {actual_output}\")\n                \n        except Exception as e:\n            test_results.append(f\"Test {i+1}: ERROR - {str(e)}\")\n    \n    # Calculate score\n    score = passed_tests / len(test_cases)\n    details = \"\\n\".join(test_results)\n    \n    return score, f\"Passed {passed_tests}/{len(test_cases)} tests.\\n{details}\"\n\ndef extract_code_blocks(text: str) -> List[str]:\n    \"\"\"Extract Python code blocks from markdown or plain text.\"\"\"\n    # Look for markdown code blocks\n    markdown_pattern = r'```(?:python)?\\s*([\\s\\S]*?)\\s*```'\n    import re\n    markdown_blocks = re.findall(markdown_pattern, text)\n    \n    if markdown_blocks:\n        return markdown_blocks\n    \n    # If no markdown blocks found, treat the entire text as code\n    # (after removing potential explanations)\n    lines = text.split('\\n')\n    code_lines = []\n    in_code = False\n    \n    for line in lines:\n        if line.strip().startswith('def ') or line.strip().startswith('class '):\n            in_code = True\n        \n        if in_code:\n            code_lines.append(line)\n    \n    if code_lines:\n        return ['\\n'.join(code_lines)]\n    \n    # Last resort: return the whole text as a potential code block\n    return [text]\n```\n\n\n## Implementation Details\n\n### Core Components\n\n#### 1. `config_manager.py`\nUpdated to handle the new `results.json` structure.\n\n```python\nimport os\nimport json\nimport yaml\nfrom typing import Dict, List, Any, Optional\n\nclass ConfigManager:\n    def __init__(self, base_path: str = None):\n        self.base_path = base_path or os.path.dirname(os.path.abspath(__file__))\n    \n    def load_results(self) -> Dict[str, Any]:\n        path = os.path.join(self.base_path, \"results.json\")\n        if not os.path.exists(path):\n            return {\"version\": \"1.0\", \"results\": {}}\n        with open(path, 'r', encoding='utf-8') as f:\n            return json.load(f)\n    \n    def save_results(self, results: Dict[str, Any]) -> None:\n        path = os.path.join(self.base_path, \"results.json\")\n        with open(path, 'w', encoding='utf-8') as f:\n            json.dump(results, f, indent=2)\n    \n    def get_pending_tests(self, llm_id: str, test_ids: Optional[List[str]] = None) -> List[str]:\n        results = self.load_results()\n        completed_tests = set(result[\"test_id\"] for result in results.get(\"results\", {}).get(llm_id, []))\n        all_tests = self.load_tests()\n        available_tests = [test[\"test_id\"] for test in all_tests.get(\"tests\", [])]\n        if test_ids:\n            available_tests = [tid for tid in available_tests if tid in test_ids]\n        return [tid for tid in available_tests if tid not in completed_tests]\n    \n    def load_test_classes(self) -> Dict[str, Any]:\n        \"\"\"Load test classes from YAML configuration.\"\"\"\n        path = os.path.join(self.base_path, \"test_classes.yaml\")\n        with open(path, 'r', encoding='utf-8') as f:\n            return yaml.safe_load(f)\n    \n    def load_tests(self) -> Dict[str, Any]:\n        \"\"\"Load test instances from JSON configuration.\"\"\"\n        path = os.path.join(self.base_path, \"tests.json\")\n        with open(path, 'r', encoding='utf-8') as f:\n            return json.load(f)\n    \n    def load_llms(self) -> Dict[str, Any]:\n        \"\"\"Load LLM configurations from JSON.\"\"\"\n        path = os.path.join(self.base_path, \"llms.json\")\n        with open(path, 'r', encoding='utf-8') as f:\n            return json.load(f)\n```\n\n#### 2. `test_runner.py`\nUses provider-specific handlers for LLMs.\n\n```python\nimport time\nimport datetime\nfrom typing import Dict, Any\n\nclass LLMProvider:\n    def generate_response(self, prompt: str, system_prompt: str, parameters: dict) -> str:\n        raise NotImplementedError()\n\nclass OllamaProvider(LLMProvider):\n    def generate_response(self, prompt: str, system_prompt: str, parameters: dict) -> str:\n        # Placeholder for Ollama-specific logic (currently OpenAI-compatible)\n        return \"Simulated Ollama response\"\n\nclass OpenAIProvider(LLMProvider):\n    def generate_response(self, prompt: str, system_prompt: str, parameters: dict) -> str:\n        # Placeholder for OpenAI-specific logic\n        return \"Simulated OpenAI response\"\n\nclass TestRunner:\n    def __init__(self, config_manager):\n        self.config_manager = config_manager\n        self.llm_providers = {\n            \"ollama\": OllamaProvider(),\n            \"openai\": OpenAIProvider(),\n            \"llm7\": LLMProvider()  # Placeholder for future custom API\n        }\n    \n    def run_test(self, test_id: str, llm_id: str) -> Dict[str, Any]:\n        llm_config = next((llm for llm in self.config_manager.load_llms()[\"llms\"] if f\"{llm['provider']}:{llm['name']}\" == llm_id), None)\n        if not llm_config:\n            raise ValueError(f\"LLM {llm_id} not found\")\n        \n        provider = self.llm_providers.get(llm_config[\"provider\"])\n        if not provider:\n            raise ValueError(f\"Provider {llm_config['provider']} not supported\")\n        \n        test = next((t for t in self.config_manager.load_tests()[\"tests\"] if t[\"test_id\"] == test_id), None)\n        if not test:\n            raise ValueError(f\"Test {test_id} not found\")\n        \n        # Simplified prompt generation (full logic omitted for brevity)\n        prompt = test[\"class_id\"]  # Replace with actual prompt generation\n        start_time = time.time()\n        response = provider.generate_response(prompt, llm_config.get(\"base_system_prompt\", \"\"), llm_config.get(\"parameters\", {}))\n        execution_time_ms = int((time.time() - start_time) * 1000)\n        \n        # Simplified evaluation (full logic in evaluators)\n        score, details = 1.0, \"Placeholder evaluation\"\n        \n        result = {\n            \"test_id\": test_id,\n            \"score\": score,\n            \"details\": details,\n            \"response\": response,\n            \"timestamp\": datetime.datetime.now().isoformat(),\n            \"execution_time_ms\": execution_time_ms\n        }\n        \n        results = self.config_manager.load_results()\n        if llm_id not in results[\"results\"]:\n            results[\"results\"][llm_id] = []\n        results[\"results\"][llm_id].append(result)\n        self.config_manager.save_results(results)\n        \n        return result\n```\n\n"
  },
  {
    "filename": "llm_manager.py",
    "language": "python",
    "content": "import os\nimport json\nimport requests\nimport time\nimport re\nfrom typing import Dict, Any, Optional, Tuple\n\nclass LLMManager:\n    \"\"\"\n    Handles all LLM API interactions for poor_bench. Add LLM providers here as needed.\n    Usage:\n        response, exec_time_ms = LLMManager().run(\n            llm_config, prompt, system_prompt, extra_params={}\n        )\n    llm_config is a dict from llms.json; prompt is user/work prompt; system_prompt may be from test class.\n    \"\"\"\n    def __init__(self):\n        self.provider_handlers = {\n            \"openai\": self._call_openai,\n            \"ollama\": self._call_ollama\n        }\n\n    def run(\n        self, \n        llm_config: Dict[str, Any], \n        prompt: str,\n        system_prompt: Optional[str] = None,\n        extra_params: Optional[Dict[str, Any]] = None\n    ) -> Tuple[str, int]:\n        \"\"\"\n        Runs a prompt against the given LLM config, returning response text and execution time ms.\n        \"\"\"\n        provider = llm_config.get(\"provider\")\n        handler = self.provider_handlers.get(provider)\n        if handler is None:\n            raise ValueError(f\"Unknown/unsupported provider '{provider}' in LLM config.\")\n        params = dict(llm_config.get('parameters', {}))\n        if extra_params:\n            params.update(extra_params)\n        start = time.time()\n        try:\n            completion = handler(\n                llm_config=llm_config,\n                prompt=prompt,\n                system_prompt=system_prompt,\n                parameters=params\n            )\n        except Exception as e:\n            completion = f\"[API ERROR] {e}\"\n        exec_time = int((time.time() - start)*1000)\n        return completion, exec_time\n\n    def _call_openai(\n        self,\n        llm_config: Dict[str, Any],\n        prompt: str,\n        system_prompt: Optional[str],\n        parameters: Dict[str, Any]\n    ) -> str:\n        endpoint = llm_config[\"endpoint\"].rstrip('/')\n        api_key_env = llm_config.get(\"api_key_env\", \"OPENAI_API_KEY\")\n        api_key = os.environ.get(api_key_env)\n        model = llm_config[\"name\"] if ':' not in llm_config[\"name\"] else llm_config[\"name\"].split(':')[0]\n        messages = []\n        # OpenAI expects messages for chat; use system/user as needed\n        if system_prompt:\n            messages.append({\"role\": \"system\", \"content\": system_prompt})\n        elif llm_config.get(\"base_system_prompt\"):\n            messages.append({\"role\": \"system\", \"content\": llm_config[\"base_system_prompt\"]})\n        messages.append({\"role\": \"user\", \"content\": prompt})\n\n        payload = {\n            \"model\": model,\n            \"messages\": messages,\n        }\n        payload.update({k: v for k, v in parameters.items() \n                        if k not in (\"top_k\", \"api_key_env\", \"endpoint\")})\n        headers = {\"Content-Type\": \"application/json\"}\n        if api_key:\n            headers[\"Authorization\"] = f\"Bearer {api_key}\"\n        try:\n            resp = requests.post(f\"{endpoint}/chat/completions\", headers=headers, json=payload, timeout=60)\n            resp.raise_for_status()\n        except Exception as e:\n            raise RuntimeError(f\"OpenAI API call failed: {e}\")\n        data = resp.json()\n        # Defensive: Try to extract message content\n        try:\n            return data[\"choices\"][0][\"message\"][\"content\"]\n        except Exception:\n            return json.dumps(data)\n\n    def _call_ollama(\n        self,\n        llm_config: Dict[str, Any],\n        prompt: str,\n        system_prompt: Optional[str],\n        parameters: Dict[str, Any]\n    ) -> str:\n        endpoint = llm_config[\"endpoint\"].rstrip('/')\n        model = llm_config[\"name\"]\n\n        # If system_prompt is present, send as part of the request.\n        request_json = {\n            \"model\": model,\n            \"messages\": [],\n        }\n        if system_prompt or llm_config.get(\"base_system_prompt\"):\n            syspt = system_prompt or llm_config.get(\"base_system_prompt\")\n            request_json[\"messages\"].append({\"role\": \"system\", \"content\": syspt})\n        request_json[\"messages\"].append({\"role\": \"user\", \"content\": prompt})\n        for p in (\"temperature\", \"top_p\", \"max_tokens\", \"seed\"):\n            if p in parameters:\n                request_json[p] = parameters[p]\n        # User may want to override ollama-specific fields\n        for k, v in parameters.items():\n            if k not in request_json:\n                request_json[k] = v\n        try:\n            resp = requests.post(f\"{endpoint}/chat\", json=request_json, timeout=60)\n            resp.raise_for_status()\n        except Exception as e:\n            raise RuntimeError(f\"Ollama API call failed: {e}\")\n        data = resp.json()\n        if \"message\" in data and \"content\" in data[\"message\"]:\n            return data[\"message\"][\"content\"]\n        elif \"response\" in data:\n            # old ollama API\n            return data[\"response\"]\n        else:\n            return json.dumps(data)\n\n    @staticmethod\n    def llm_id(llm_config: Dict[str, Any]) -> str:\n        \"\"\"Returns canonical LLM ID for results: provider:name\"\"\"\n        return f\"{llm_config['provider']}:{llm_config['name']}\"\n"
  }
]
```

**[FORMAT SPEC]**
- You are a code generator tasked with modifying or creating files to implement the specified features.
- Output a structured JSON array:
  ```json
  [
    {
      "filename": "<filename>",
      "language": "<language>",
      "short": "<summary>",
      "detailed": "<longer description>",
      "content": "..."
    }
  ]
  ```
- "content": The file’s complete code as a string, with special characters (e.g., quotes, newlines) properly escaped to ensure valid JSON. 


Response:
```json
[
  {
    "filename": "poor_bench/__init__.py",
    "language": "python",
    "short": "Initialization file for the poor_bench package.",
    "detailed": "Makes the poor_bench directory a Python package. This file can be empty or can be used to expose key components of the package at the top level.",
    "content": "# poor_bench package\n"
  },
  {
    "filename": "poor_bench/evaluators/__init__.py",
    "language": "python",
    "short": "Initialization file for the evaluators sub-package.",
    "detailed": "Makes the evaluators directory a Python sub-package. This file is typically empty.",
    "content": "# poor_bench.evaluators package\n"
  },
  {
    "filename": "poor_bench/test_classes.yaml",
    "language": "yaml",
    "short": "Configuration file defining test classes, prompt templates, and evaluation modules.",
    "detailed": "Contains definitions for different categories of tests, including their descriptions, system prompts, work prompt templates (with LLM-specific overrides), and default evaluation module configurations.",
    "content": "version: \"1.0\"\ntest_classes:\n  - id: \"math_problems\"\n    description: \"Tests for solving mathematical problems\"\n    system_prompt: \"You are a math expert. Provide clear solutions.\"\n    work_prompt_template:\n      default: \"Solve this math problem and return your answer in JSON format with key 'answer': {problem}\"\n      overrides:\n        \"ollama:gemma3:1b-it-qat\": \"Solve this and return JSON with 'answer': {problem}\"\n    evaluation_module:\n      name: \"math_evaluator\"\n      parameters:\n        precision: 2\n        expected_format: \"json\"\n\n  - id: \"sentiment_analysis\"\n    description: \"Tests for determining sentiment of texts\"\n    system_prompt: \"You are a sentiment analysis expert.\"\n    work_prompt_template:\n      default: \"Analyze the sentiment of these texts and return a JSON list of sentiments ['positive', 'negative', 'neutral']: {texts}\"\n    evaluation_module:\n      name: \"sentiment_evaluator\"\n      parameters:\n        response_format: \"list_of_sentiments\"\n\n  - id: \"python_coding\"\n    description: \"Tests for writing Python code to solve programming problems\"\n    system_prompt: \"You are an expert Python programmer. Write clean, efficient, and well-documented code.\"\n    work_prompt_template:\n      default: \"Write a Python function that {task}. Include docstrings and comments where appropriate. Ensure the function is self-contained and does not rely on external libraries unless specified.\"\n    evaluation_module:\n      name: \"python_evaluator\"\n      parameters: {}\n"
  },
  {
    "filename": "poor_bench/tests.json",
    "language": "json",
    "short": "Configuration file storing specific test instances linked to test classes.",
    "detailed": "Contains an array of test objects, each with a unique ID, a link to a test class, difficulty level, test content (text or texts), and specific evaluation module parameters for that instance.",
    "content": "{\n  \"version\": \"1.0\",\n  \"tests\": [\n    {\n      \"test_id\": \"math_problems_level1_001\",\n      \"class_id\": \"math_problems\",\n      \"level\": 1,\n      \"text\": \"What is 2 + 2?\",\n      \"evaluation_module\": {\n        \"name\": \"math_evaluator\",\n        \"parameters\": {\n          \"expected_answer\": \"4\",\n          \"precision\": 0\n        }\n      }\n    },\n    {\n      \"test_id\": \"math_problems_level1_002\",\n      \"class_id\": \"math_problems\",\n      \"level\": 1,\n      \"text\": \"What is 10 / 4?\",\n      \"evaluation_module\": {\n        \"name\": \"math_evaluator\",\n        \"parameters\": {\n          \"expected_answer\": \"2.5\",\n          \"precision\": 1\n        }\n      }\n    },\n    {\n      \"test_id\": \"sentiment_analysis_level1_001\",\n      \"class_id\": \"sentiment_analysis\",\n      \"level\": 1,\n      \"texts\": [\n        \"I love this product!\",\n        \"This is the worst experience ever.\",\n        \"It's okay, nothing special.\"\n      ],\n      \"evaluation_module\": {\n        \"name\": \"sentiment_evaluator\",\n        \"parameters\": {\n          \"expected_sentiments\": [\"positive\", \"negative\", \"neutral\"]\n        }\n      }\n    },\n    {\n      \"test_id\": \"python_coding_level2_001\",\n      \"class_id\": \"python_coding\",\n      \"level\": 2,\n      \"text\": \"calculates the factorial of a non-negative integer\",\n      \"evaluation_module\": {\n        \"name\": \"python_evaluator\",\n        \"parameters\": {\n          \"test_cases\": [\n            {\"input\": 5, \"expected_output\": 120},\n            {\"input\": 0, \"expected_output\": 1},\n            {\"input\": 1, \"expected_output\": 1}\n          ],\n          \"function_name\": \"factorial\",\n          \"execution_timeout\": 2\n        }\n      }\n    },\n    {\n      \"test_id\": \"python_coding_level1_001\",\n      \"class_id\": \"python_coding\",\n      \"level\": 1,\n      \"text\": \"adds two numbers\",\n      \"evaluation_module\": {\n        \"name\": \"python_evaluator\",\n        \"parameters\": {\n          \"test_cases\": [\n            {\"input\": [2, 3], \"expected_output\": 5},\n            {\"input\": [-1, 1], \"expected_output\": 0},\n            {\"input\": [0, 0], \"expected_output\": 0}\n          ],\n          \"function_name\": \"add_numbers\",\n          \"execution_timeout\": 1\n        }\n      }\n    }\n  ]\n}\n"
  },
  {
    "filename": "poor_bench/llms.json",
    "language": "json",
    "short": "Configuration file defining Large Language Models (LLMs) and their parameters.",
    "detailed": "Contains a list of LLM objects, each specifying the provider, model name, API endpoint, optional base system prompt, API key environment variable (if needed), and other LLM-specific parameters like temperature or max tokens.",
    "content": "{\n  \"llms\": [\n    {\n      \"provider\": \"ollama\",\n      \"name\": \"gemma:2b-instruct-q4_0\", \n      \"endpoint\": \"http://127.0.0.1:11434/v1\",\n      \"base_system_prompt\": \"You are a helpful AI assistant. Provide concise and accurate answers.\",\n      \"parameters\": {\n        \"temperature\": 0.7,\n        \"top_p\": 0.9\n      }\n    },\n    {\n      \"provider\": \"openai\",\n      \"name\": \"gpt-3.5-turbo\",\n      \"endpoint\": \"https://api.openai.com/v1\",\n      \"api_key_env\": \"OPENAI_API_KEY\",\n      \"base_system_prompt\": \"You are an advanced AI assistant.\",\n      \"parameters\": {\n        \"temperature\": 0.5,\n        \"max_tokens\": 1024\n      }\n    }\n  ]\n}\n"
  },
  {
    "filename": "poor_bench/results.json",
    "language": "json",
    "short": "File for storing test results, organized by LLM ID.",
    "detailed": "Logs results from test runs. It includes a version and a dictionary where keys are LLM identifiers and values are lists of result objects. Each result object contains test ID, score, details, raw LLM response, timestamp, and execution time.",
    "content": "{\n  \"version\": \"1.0\",\n  \"results\": {}\n}\n"
  },
  {
    "filename": "poor_bench/evaluators/math_evaluator.py",
    "language": "python",
    "short": "Evaluation module for math problems.",
    "detailed": "Handles responses for math problems, typically expecting JSON output. It compares the LLM's answer to an expected answer with a specified precision.",
    "content": "import json\nfrom decimal import Decimal, InvalidOperation\n\ndef evaluate(response: str, parameters: dict) -> tuple[float, str]:\n    expected = parameters.get(\"expected_answer\")\n    precision = parameters.get(\"precision\", 2)  # Default precision if not specified\n    # expected_format is from test_classes.yaml, not typically overridden per test instance for this evaluator\n    # For this simple version, we assume JSON if 'answer' key is sought.\n\n    try:\n        # Attempt to parse as JSON first, as per common use case\n        response_data = json.loads(response)\n        if isinstance(response_data, dict):\n            actual = response_data.get(\"answer\")\n            if actual is None:\n                # Fallback: maybe the response IS the answer, not wrapped in JSON\n                # This is a heuristic. A stricter evaluator might fail here.\n                actual = response # Treat the whole response as the potential answer string\n        elif isinstance(response_data, (int, float, str)):\n            # If JSON parses to a direct number or string, use that.\n            actual = response_data\n        else:\n            # Unrecognized JSON structure\n            actual = response # Fallback to raw response\n            \n    except json.JSONDecodeError:\n        # If not JSON, assume the raw response string is the answer\n        actual = response\n    except Exception:\n        return 0.0, \"Error processing response structure.\"\n\n    if expected is None:\n        return 0.0, \"Missing 'expected_answer' in evaluation parameters.\"\n\n    try:\n        # Ensure expected and actual are strings before Decimal conversion for consistency\n        expected_decimal = Decimal(str(expected))\n        actual_decimal = Decimal(str(actual))\n        \n        # Quantize both to the same precision before comparison\n        # Precision is number of decimal places, so '1e-2' for 2 places, '1e-0' for 0 places.\n        quantizer = Decimal('1e-' + str(precision))\n        \n        if actual_decimal.quantize(quantizer) == expected_decimal.quantize(quantizer):\n            return 1.0, f\"Correct. Expected: {expected_decimal}, Got: {actual_decimal}\"\n        else:\n            return 0.0, f\"Incorrect. Expected: {expected_decimal}, Got: {actual_decimal}\"\n    except InvalidOperation:\n        # Handle cases where conversion to Decimal fails (e.g., non-numeric strings)\n        # Simple string comparison as a last resort if numbers are not interpretable\n        if str(actual).strip() == str(expected).strip():\n            return 1.0, f\"Correct (non-numeric match). Expected: {expected}, Got: {actual}\"\n        return 0.0, f\"Invalid numeric format or non-matching non-numeric. Expected: {expected}, Got: {actual}\"\n    except Exception as e:\n        return 0.0, f\"Error during numeric comparison: {str(e)}\"\n"
  },
  {
    "filename": "poor_bench/evaluators/sentiment_evaluator.py",
    "language": "python",
    "short": "Evaluation module for sentiment analysis tasks.",
    "detailed": "Evaluates LLM responses for sentiment analysis, expecting a JSON list of sentiments. It compares the identified sentiments against a list of expected sentiments.",
    "content": "import json\n\ndef evaluate(response: str, parameters: dict) -> tuple[float, str]:\n    expected_sentiments = parameters.get(\"expected_sentiments\", [])\n    # response_format is from test_classes.yaml, typically 'list_of_sentiments'\n    \n    if not expected_sentiments:\n        return 0.0, \"No 'expected_sentiments' provided in parameters.\"\n\n    try:\n        actual_sentiments = json.loads(response)\n        if not isinstance(actual_sentiments, list):\n            return 0.0, f\"Response must be a JSON list. Got: {type(actual_sentiments)}\"\n    except json.JSONDecodeError:\n        return 0.0, f\"Invalid JSON in response: {response[:100]}...\"\n    except Exception as e:\n        return 0.0, f\"Error parsing response: {str(e)}\"\n    \n    if len(actual_sentiments) != len(expected_sentiments):\n        return 0.0, f\"Expected {len(expected_sentiments)} sentiments, got {len(actual_sentiments)}.\"\n    \n    correct_count = 0\n    details = []\n    for i, (actual, expected) in enumerate(zip(actual_sentiments, expected_sentiments)):\n        if str(actual).strip().lower() == str(expected).strip().lower():\n            correct_count += 1\n            details.append(f\"Item {i+1}: Correct ('{expected}')\")\n        else:\n            details.append(f\"Item {i+1}: Incorrect. Expected: '{expected}', Got: '{actual}'\")\n            \n    if not expected_sentiments: # Should be caught earlier, but defensive\n        score = 0.0\n    else:\n        score = float(correct_count) / len(expected_sentiments)\n        \n    detail_str = f\"Correctly identified {correct_count} out of {len(expected_sentiments)} sentiments. Details:\\n\" + \"\\n\".join(details)\n    return score, detail_str\n"
  },
  {
    "filename": "poor_bench/evaluators/python_evaluator.py",
    "language": "python",
    "short": "Evaluation module for Python coding tasks.",
    "detailed": "Evaluates Python code generated by an LLM. It extracts code, checks syntax, executes it against a set of test cases, and compares outputs. Supports function name checking and execution timeout.",
    "content": "import sys\nimport io\nimport traceback\nimport ast\nimport time\nfrom contextlib import redirect_stdout, redirect_stderr\nfrom typing import Dict, List, Any, Tuple\nimport re\n\ndef extract_code_blocks(text: str) -> List[str]:\n    \"\"\"Extract Python code blocks from markdown or plain text.\"\"\"\n    # Look for markdown code blocks\n    markdown_pattern = r'```(?:python)?\\s*([\\s\\S]*?)\\s*```'\n    markdown_blocks = re.findall(markdown_pattern, text)\n    \n    if markdown_blocks:\n        return markdown_blocks\n    \n    # If no markdown blocks found, try to infer based on common Python keywords\n    lines = text.split('\\n')\n    code_lines = []\n    in_code = False\n    \n    # A simple heuristic: if a line starts with def or class, assume it's code from there\n    # This is not foolproof but better than nothing for plain text responses.\n    for i, line in enumerate(lines):\n        stripped_line = line.strip()\n        if stripped_line.startswith('def ') or stripped_line.startswith('class '):\n            in_code = True\n            code_lines.extend(lines[i:]) # Take this line and all subsequent lines\n            break \n        # Heuristic: if it looks like an import statement and we are not in code yet\n        if not in_code and (stripped_line.startswith('import ') or stripped_line.startswith('from ')):\n            # Tentatively start collecting if it seems like a script beginning\n            # This part is tricky without more context; could collect non-code if LLM explains imports first.\n            # For now, rely on def/class as stronger signals.\n            pass \n            \n    if code_lines:\n        return ['\\n'.join(code_lines)]\n    \n    # Last resort: return the whole text as a potential code block if it contains common keywords\n    # This helps if the LLM just returns code without markdown or clear 'def'/'class' at the start of a block\n    if any(kw in text for kw in ['def ', 'class ', 'import ', 'return ', 'for ', 'while ']):\n        return [text]\n    return []\n\ndef evaluate(response: str, parameters: dict) -> tuple[float, str]:\n    \"\"\"\n    Evaluates Python code solutions.\n    \n    Args:\n        response: String, LLM's Python code solution.\n        parameters: Dict with keys:\n            - test_cases: List of dicts with 'input' and 'expected_output' keys\n            - function_name: Expected name of the function (optional)\n            - execution_timeout: Maximum execution time in seconds (default: 5)\n    \n    Returns:\n        Tuple[float, str]: Score (0.0 to 1.0) and explanation.\n    \"\"\"\n    code_blocks = extract_code_blocks(response)\n    if not code_blocks:\n        return 0.0, \"No Python code found in the response.\"\n    \n    code = max(code_blocks, key=len) # Use the largest block, assuming it's the main solution\n    \n    try:\n        ast.parse(code)\n    except SyntaxError as e:\n        return 0.0, f\"Syntax error in extracted code: {str(e)}\\nCode: {code[:500]}...\"\n    \n    test_cases = parameters.get(\"test_cases\", [])\n    function_name_param = parameters.get(\"function_name\")\n    timeout = parameters.get(\"execution_timeout\", 5)\n    \n    if not test_cases:\n        return 0.5, \"Code syntax is valid, but no test cases provided for functional verification.\"\n    \n    namespace = {}\n    \n    try:\n        # Create a restricted global namespace for exec\n        # Allow builtins but nothing else from the current global scope\n        restricted_globals = {'__builtins__': __builtins__}\n        exec(code, restricted_globals, namespace)\n    except Exception as e:\n        # Capture traceback for more detailed error reporting\n        tb_str = traceback.format_exc()\n        return 0.0, f\"Error executing user code: {str(e)}\\nTraceback:\\n{tb_str}\\nCode: {code[:500]}...\"\n\n    # Determine the function to test\n    target_function = None\n    if function_name_param:\n        if function_name_param not in namespace:\n            return 0.0, f\"Function '{function_name_param}' not found in the executed code.\"\n        target_function = namespace[function_name_param]\n    else:\n        # Auto-detect function if name not provided\n        callable_functions = [obj for name, obj in namespace.items() \n                              if callable(obj) and not name.startswith('__') and hasattr(obj, '__module__') and obj.__module__ == '<string>'] # Check __module__ to prefer user-defined functions\n        if not callable_functions:\n            return 0.0, \"No user-defined functions found in the code.\"\n        if len(callable_functions) > 1:\n             # If multiple functions, this could be ambiguous. For now, pick the first one.\n             # A better approach might be to require function_name or handle multiple entry points.\n            pass # proceed with the first one\n        target_function = callable_functions[0]\n        function_name_param = target_function.__name__ # Update for reporting\n\n    if not callable(target_function):\n        return 0.0, f\"'{function_name_param}' is not a callable function.\"\n\n    passed_tests = 0\n    test_results_details = []\n    \n    for i, tc in enumerate(test_cases):\n        input_val = tc.get(\"input\")\n        expected_output = tc.get(\"expected_output\")\n        \n        stdout_capture = io.StringIO()\n        stderr_capture = io.StringIO()\n        \n        actual_output = None\n        error_occurred = False\n        \n        try:\n            start_time = time.perf_counter()\n            with redirect_stdout(stdout_capture), redirect_stderr(stderr_capture):\n                if input_val is None: # For functions that take no arguments\n                    actual_output = target_function()\n                elif isinstance(input_val, list): # Positional arguments\n                    actual_output = target_function(*input_val)\n                elif isinstance(input_val, dict): # Keyword arguments\n                    actual_output = target_function(**input_val)\n                else: # Single argument\n                    actual_output = target_function(input_val)\n            duration = time.perf_counter() - start_time\n\n            if duration > timeout:\n                test_results_details.append(f\"Test Case {i+1} (Input: {input_val}): TIMEOUT (>{timeout}s)\")\n                continue\n\n            # Check for errors printed to stderr during execution\n            err_output = stderr_capture.getvalue()\n            if err_output:\n                test_results_details.append(f\"Test Case {i+1} (Input: {input_val}): RUNTIME ERROR - Stderr: {err_output.strip()}\")\n                continue\n                \n            if actual_output == expected_output:\n                passed_tests += 1\n                test_results_details.append(f\"Test Case {i+1} (Input: {input_val}): PASS\")\n            else:\n                test_results_details.append(f\"Test Case {i+1} (Input: {input_val}): FAIL - Expected: {expected_output}, Got: {actual_output}\")\n        \n        except Exception as e:\n            tb_str = traceback.format_exc(limit=1) # Keep traceback short\n            test_results_details.append(f\"Test Case {i+1} (Input: {input_val}): EXECUTION ERROR - {str(e)}\\n{tb_str.strip()}\")\n            error_occurred = True # To mark this test as failed\n\n    score = passed_tests / len(test_cases) if test_cases else 0.0\n    summary = f\"Function '{function_name_param}': Passed {passed_tests}/{len(test_cases)} test cases.\"\n    full_details = summary + \"\\n\" + \"\\n\".join(test_results_details)\n    \n    return score, full_details\n"
  },
  {
    "filename": "poor_bench/config_manager.py",
    "language": "python",
    "short": "Manages loading and saving of configuration files and results.",
    "detailed": "Provides methods to load test classes (YAML), test instances (JSON), LLM configurations (JSON), and test results (JSON). It also includes functionality to save results and determine pending tests for a given LLM.",
    "content": "import os\nimport json\nimport yaml\nfrom typing import Dict, List, Any, Optional\n\nclass ConfigManager:\n    def __init__(self, base_path: str = None):\n        if base_path is None:\n            # Default base_path to the directory containing this config_manager.py file\n            self.base_path = os.path.dirname(os.path.abspath(__file__))\n        else:\n            self.base_path = base_path\n    \n    def _load_json(self, filename: str) -> Dict[str, Any]:\n        path = os.path.join(self.base_path, filename)\n        if not os.path.exists(path):\n            # For results.json, return an empty structure if it doesn't exist\n            if filename == \"results.json\":\n                return {\"version\": \"1.0\", \"results\": {}}\n            raise FileNotFoundError(f\"Configuration file not found: {path}\")\n        with open(path, 'r', encoding='utf-8') as f:\n            return json.load(f)\n\n    def _save_json(self, filename: str, data: Dict[str, Any]) -> None:\n        path = os.path.join(self.base_path, filename)\n        with open(path, 'w', encoding='utf-8') as f:\n            json.dump(data, f, indent=2)\n\n    def _load_yaml(self, filename: str) -> Dict[str, Any]:\n        path = os.path.join(self.base_path, filename)\n        if not os.path.exists(path):\n            raise FileNotFoundError(f\"Configuration file not found: {path}\")\n        with open(path, 'r', encoding='utf-8') as f:\n            return yaml.safe_load(f)\n\n    def load_results(self) -> Dict[str, Any]:\n        return self._load_json(\"results.json\")\n    \n    def save_results(self, results_data: Dict[str, Any]) -> None:\n        self._save_json(\"results.json\", results_data)\n    \n    def get_pending_tests(self, llm_id_str: str, target_test_ids: Optional[List[str]] = None) -> List[Dict[str, Any]]:\n        all_tests_config = self.load_tests()\n        all_test_instances = all_tests_config.get(\"tests\", [])\n        \n        current_results_data = self.load_results()\n        llm_results = current_results_data.get(\"results\", {}).get(llm_id_str, [])\n        completed_test_ids = {result[\"test_id\"] for result in llm_results}\n        \n        pending_tests = [] \n        if target_test_ids is not None:\n            # Filter by specific test IDs if provided\n            for test_instance in all_test_instances:\n                if test_instance[\"test_id\"] in target_test_ids and test_instance[\"test_id\"] not in completed_test_ids:\n                    pending_tests.append(test_instance)\n        else:\n            # Otherwise, consider all tests not yet completed\n            for test_instance in all_test_instances:\n                if test_instance[\"test_id\"] not in completed_test_ids:\n                    pending_tests.append(test_instance)\n        return pending_tests\n\n    def load_test_classes(self) -> Dict[str, Any]:\n        \"\"\"Load test classes from YAML configuration.\"\"\"\n        return self._load_yaml(\"test_classes.yaml\")\n    \n    def load_tests(self) -> Dict[str, Any]:\n        \"\"\"Load test instances from JSON configuration.\"\"\"\n        return self._load_json(\"tests.json\")\n    \n    def get_test_instance_by_id(self, test_id: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Retrieve a single test instance by its ID.\"\"\"\n        all_tests = self.load_tests().get(\"tests\", [])\n        for test in all_tests:\n            if test[\"test_id\"] == test_id:\n                return test\n        return None\n\n    def load_llms(self) -> Dict[str, Any]:\n        \"\"\"Load LLM configurations from JSON.\"\"\"\n        return self._load_json(\"llms.json\")\n\n    def get_llm_config_by_id(self, llm_id_str: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Retrieve a single LLM configuration by its ID (provider:name).\"\"\"\n        all_llms = self.load_llms().get(\"llms\", [])\n        for llm_config in all_llms:\n            if f\"{llm_config['provider']}:{llm_config['name']}\" == llm_id_str:\n                return llm_config\n        return None\n\n    def get_test_class_by_id(self, class_id: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Retrieve a single test class configuration by its ID.\"\"\"\n        all_test_classes = self.load_test_classes().get(\"test_classes\", [])\n        for tc in all_test_classes:\n            if tc[\"id\"] == class_id:\n                return tc\n        return None\n"
  },
  {
    "filename": "poor_bench/test_runner.py",
    "language": "python",
    "short": "Coordinates the execution of individual tests against specified LLMs.",
    "detailed": "The TestRunner class is responsible for running a single test: it loads test data, LLM configuration, and test class details; constructs the appropriate prompt; uses LLMManager to get the LLM's response; dynamically loads and calls an evaluator module; and saves the result via ConfigManager.",
    "content": "import time\nimport datetime\nimport importlib\nimport json\nimport re\nfrom typing import Dict, Any, Tuple, Optional\n\nfrom .config_manager import ConfigManager\nfrom .llm_manager import LLMManager # Assuming llm_manager.py is in the same directory\n\nclass TestRunner:\n    def __init__(self, config_manager: ConfigManager, llm_manager: LLMManager):\n        self.config_manager = config_manager\n        self.llm_manager = llm_manager\n\n    def _get_prompt_template(self, test_class_config: Dict[str, Any], llm_id_str: str) -> str:\n        template_config = test_class_config.get(\"work_prompt_template\", {})\n        if llm_id_str in template_config.get(\"overrides\", {}):\n            return template_config[\"overrides\"][llm_id_str]\n        return template_config.get(\"default\", \"\")\n\n    def _format_prompt(self, template: str, test_instance: Dict[str, Any]) -> str:\n        # Simple substitution based on available keys in test_instance\n        # More sophisticated logic might be needed if placeholders are complex\n        data_to_format = {}\n        if \"text\" in test_instance:\n            data_to_format[\"text\"] = test_instance[\"text\"]\n            # Try common placeholders if 'text' is the key\n            # This is heuristic; ideally, tests.json keys match template placeholders\n            placeholders = re.findall(r'\\{([^}]+)\\}', template)\n            if len(placeholders) == 1:\n                # If single placeholder like {problem} or {task}, map 'text' to it\n                if placeholders[0] not in [\"texts\"]: # Avoid overwriting if {texts} is explicitly used\n                    data_to_format[placeholders[0]] = test_instance[\"text\"]\n        \n        if \"texts\" in test_instance:\n            # Ensure texts are passed as a string representation (e.g., JSON string) if template expects a single string placeholder\n            # Or, if the template is designed to iterate or handle a list, this might need adjustment.\n            # For now, assume {texts} expects a string (e.g. json.dumps(test_instance[\"texts\"]))\n            # The current example for sentiment_analysis expects {texts} to be a string of a list\n            data_to_format[\"texts\"] = json.dumps(test_instance[\"texts\"])\n\n        try:\n            return template.format(**data_to_format)\n        except KeyError as e:\n            # Fallback if specific keys like {problem} or {task} are missing but {text} might work\n            if 'text' in data_to_format and 'text' not in str(e):\n                 try: \n                     return template.format(text=data_to_format['text']) \n                 except Exception:\n                     pass # If this also fails, original error is more informative\n            raise ValueError(f\"Failed to format prompt. Missing key: {e}. Template: '{template}', Data: {data_to_format}\")\n        except Exception as e:\n            raise ValueError(f\"Error formatting prompt: {e}. Template: '{template}', Data: {data_to_format}\")\n\n    def run_test(self, test_id: str, llm_id_str: str) -> Dict[str, Any]:\n        test_instance = self.config_manager.get_test_instance_by_id(test_id)\n        if not test_instance:\n            raise ValueError(f\"Test instance '{test_id}' not found.\")\n\n        llm_config = self.config_manager.get_llm_config_by_id(llm_id_str)\n        if not llm_config:\n            raise ValueError(f\"LLM configuration for '{llm_id_str}' not found.\")\n\n        test_class_config = self.config_manager.get_test_class_by_id(test_instance[\"class_id\"])\n        if not test_class_config:\n            raise ValueError(f\"Test class '{test_instance['class_id']}' not found.\")\n\n        # Prepare prompt\n        system_prompt = test_class_config.get(\"system_prompt\")\n        # LLM's base_system_prompt is handled by LLMManager if test-specific one isn't provided\n        \n        work_prompt_template_str = self._get_prompt_template(test_class_config, llm_id_str)\n        if not work_prompt_template_str:\n             raise ValueError(f\"No work prompt template found for test class '{test_class_config['id']}' and LLM '{llm_id_str}'.\")\n        \n        work_prompt = self._format_prompt(work_prompt_template_str, test_instance)\n\n        # Get LLM response\n        response_text, execution_time_ms = self.llm_manager.run(\n            llm_config=llm_config,\n            prompt=work_prompt,\n            system_prompt=system_prompt\n        )\n\n        # Evaluate response\n        # Merge parameters: instance overrides class defaults\n        eval_module_class_config = test_class_config.get(\"evaluation_module\", {})\n        eval_module_instance_config = test_instance.get(\"evaluation_module\", {})\n        \n        eval_module_name = eval_module_instance_config.get(\"name\") or eval_module_class_config.get(\"name\")\n        if not eval_module_name:\n            raise ValueError(f\"Evaluation module name not specified for test '{test_id}'.\")\n\n        eval_parameters = dict(eval_module_class_config.get(\"parameters\", {}))\n        eval_parameters.update(eval_module_instance_config.get(\"parameters\", {}))\n\n        score = 0.0\n        eval_details = \"Evaluation skipped: Error loading/running evaluator.\"\n        try:\n            # Dynamically import the evaluator module\n            # Assumes evaluators are in poor_bench.evaluators package\n            evaluator_module_path = f\"poor_bench.evaluators.{eval_module_name}\"\n            module = importlib.import_module(evaluator_module_path)\n            score, eval_details = module.evaluate(response_text, eval_parameters)\n        except ImportError:\n            eval_details = f\"Evaluation module '{evaluator_module_path}' not found.\"\n            print(f\"ERROR: {eval_details}\")\n        except AttributeError:\n            eval_details = f\"'evaluate' function not found in module '{evaluator_module_path}'.\"\n            print(f\"ERROR: {eval_details}\")\n        except Exception as e:\n            eval_details = f\"Error during evaluation with '{eval_module_name}': {str(e)}\"\n            print(f\"ERROR: {eval_details}\")\n\n        # Construct and save result\n        result_entry = {\n            \"test_id\": test_id,\n            \"score\": float(score),\n            \"details\": eval_details,\n            \"response\": response_text,\n            \"timestamp\": datetime.datetime.now(datetime.timezone.utc).isoformat(),\n            \"execution_time_ms\": execution_time_ms\n        }\n\n        results_data = self.config_manager.load_results()\n        if llm_id_str not in results_data[\"results\"]:\n            results_data[\"results\"][llm_id_str] = []\n        \n        # Remove any previous result for this specific test_id and llm_id_str before appending new one\n        results_data[\"results\"][llm_id_str] = [\n            r for r in results_data[\"results\"][llm_id_str] if r[\"test_id\"] != test_id\n        ]\n        results_data[\"results\"][llm_id_str].append(result_entry)\n        self.config_manager.save_results(results_data)\n\n        return result_entry\n"
  },
  {
    "filename": "poor_bench/report_generator.py",
    "language": "python",
    "short": "Generates reports from test results in various formats.",
    "detailed": "The ReportGenerator class loads test results using ConfigManager and can produce reports like a console summary, CSV file, or JSON file. Reports can be filtered by LLM IDs or test IDs.",
    "content": "import json\nimport csv\nfrom typing import Dict, List, Any, Optional\nimport pandas as pd\n\nfrom .config_manager import ConfigManager\n\nclass ReportGenerator:\n    def __init__(self, config_manager: ConfigManager):\n        self.config_manager = config_manager\n\n    def _get_filtered_results(self, llm_ids: Optional[List[str]] = None, \n                                test_ids: Optional[List[str]] = None) -> Dict[str, List[Dict[str, Any]]]:\n        all_results_data = self.config_manager.load_results()\n        raw_results_map = all_results_data.get(\"results\", {})\n        \n        filtered_results_map = {}\n\n        target_llm_ids = llm_ids if llm_ids else raw_results_map.keys()\n\n        for llm_id_str, results_list in raw_results_map.items():\n            if llm_id_str not in target_llm_ids:\n                continue\n\n            if not results_list:\n                filtered_results_map[llm_id_str] = []\n                continue\n            \n            if test_ids:\n                current_llm_results = [res for res in results_list if res[\"test_id\"] in test_ids]\n            else:\n                current_llm_results = list(results_list) # Make a copy\n            \n            if current_llm_results: # Add only if there are matching results\n                 filtered_results_map[llm_id_str] = current_llm_results\n        \n        return filtered_results_map\n\n    def generate_summary_report(self, llm_ids: Optional[List[str]] = None, \n                                test_ids: Optional[List[str]] = None) -> None:\n        \"\"\"Prints a summary report to the console.\"\"\"\n        results_map = self._get_filtered_results(llm_ids, test_ids)\n        \n        if not results_map:\n            print(\"No results found for the given filters.\")\n            return\n\n        print(\"\\n--- Poor Bench Summary Report ---\")\n        for llm_id_str, results_list in results_map.items():\n            if not results_list:\n                print(f\"\\nLLM: {llm_id_str} - No results found.\")\n                continue\n\n            total_tests = len(results_list)\n            avg_score = sum(r[\"score\"] for r in results_list) / total_tests if total_tests > 0 else 0\n            avg_exec_time = sum(r[\"execution_time_ms\"] for r in results_list) / total_tests if total_tests > 0 else 0\n            \n            print(f\"\\nLLM: {llm_id_str}\")\n            print(f\"  Total Tests Run: {total_tests}\")\n            print(f\"  Average Score: {avg_score:.2f}\")\n            print(f\"  Average Execution Time: {avg_exec_time:.0f} ms\")\n            \n            # Optional: Group by test class or level if that data is easily accessible/added to results\n            # For now, a simple list of test scores:\n            # print(\"  Individual Test Scores:\")\n            # for res in sorted(results_list, key=lambda x: x['test_id']):\n            #     print(f\"    - {res['test_id']}: {res['score']:.2f} ({res['execution_time_ms']}ms)\")\n        print(\"\\n--- End of Report ---\")\n\n    def generate_csv_report(self, output_file: str, llm_ids: Optional[List[str]] = None, \n                            test_ids: Optional[List[str]] = None) -> None:\n        \"\"\"Generates a CSV report and saves it to output_file.\"\"\"\n        results_map = self._get_filtered_results(llm_ids, test_ids)\n        \n        if not results_map:\n            print(f\"No results to write to CSV for file {output_file}.\")\n            # Create an empty CSV with headers if desired, or just do nothing\n            # with open(output_file, 'w', newline='', encoding='utf-8') as f:\n            #    writer = csv.writer(f)\n            #    writer.writerow([\"llm_id\", \"test_id\", \"score\", \"execution_time_ms\", \"timestamp\", \"details\", \"response\"])\n            return\n\n        report_data = []\n        for llm_id_str, results_list in results_map.items():\n            for result in results_list:\n                report_data.append({\n                    \"llm_id\": llm_id_str,\n                    \"test_id\": result[\"test_id\"],\n                    \"score\": result[\"score\"],\n                    \"execution_time_ms\": result[\"execution_time_ms\"],\n                    \"timestamp\": result[\"timestamp\"],\n                    \"details\": result[\"details\"],\n                    \"response\": result[\"response\"]\n                })\n        \n        if not report_data:\n            print(f\"No data rows to write for CSV report {output_file}.\")\n            return\n\n        try:\n            df = pd.DataFrame(report_data)\n            df.to_csv(output_file, index=False, encoding='utf-8')\n            print(f\"CSV report generated: {output_file}\")\n        except Exception as e:\n            print(f\"Error generating CSV report: {e}\")\n\n    def generate_json_report(self, output_file: str, llm_ids: Optional[List[str]] = None, \n                             test_ids: Optional[List[str]] = None) -> None:\n        \"\"\"Generates a JSON report (essentially filtered results.json) and saves it.\"\"\"\n        results_map = self._get_filtered_results(llm_ids, test_ids)\n        \n        report_content = {\n            \"version\": self.config_manager.load_results().get(\"version\", \"1.0\"), # Use original version\n            \"report_generated_at\": datetime.datetime.now(datetime.timezone.utc).isoformat(),\n            \"filters_applied\": {\n                \"llm_ids\": llm_ids,\n                \"test_ids\": test_ids\n            },\n            \"results\": results_map\n        }\n        \n        try:\n            with open(output_file, 'w', encoding='utf-8') as f:\n                json.dump(report_content, f, indent=2)\n            print(f\"JSON report generated: {output_file}\")\n        except Exception as e:\n            print(f\"Error generating JSON report: {e}\")\n"
  },
  {
    "filename": "poor_bench/main.py",
    "language": "python",
    "short": "Main command-line interface for the Poor Bench framework.",
    "detailed": "Provides CLI commands to run tests, generate reports, and list configurations (LLMs, tests, test classes). Uses argparse for command parsing and orchestrates calls to ConfigManager, LLMManager, TestRunner, and ReportGenerator.",
    "content": "import argparse\nimport os\nimport sys\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom typing import List, Optional\n\n# Ensure the package components can be imported if main.py is run directly\n# For example, when `python poor_bench/main.py ...` is executed from `poor_ai/`\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\n#PARENT_DIR = os.path.dirname(SCRIPT_DIR)\n#sys.path.insert(0, PARENT_DIR) # Add poor_ai to path, so poor_bench is a known package\n# Better: if poor_bench is installed or PYTHONPATH is set, direct imports work.\n# For development, running `python -m poor_bench.main ...` from `poor_ai/` is preferred.\n\ntry:\n    from .config_manager import ConfigManager\n    from .llm_manager import LLMManager\n    from .test_runner import TestRunner\n    from .report_generator import ReportGenerator\nexcept ImportError:\n    # Fallback for direct script execution if not run as a module\n    # This is hacky; proper packaging or PYTHONPATH setup is better.\n    sys.path.insert(0, os.path.join(SCRIPT_DIR, '..')) # Assumes poor_bench is in poor_ai/\n    from poor_bench.config_manager import ConfigManager\n    from poor_bench.llm_manager import LLMManager\n    from poor_bench.test_runner import TestRunner\n    from poor_bench.report_generator import ReportGenerator\n\n\ndef run_tests_handler(args):\n    base_path = args.config_dir if args.config_dir else None\n    cm = ConfigManager(base_path=base_path)\n    lm = LLMManager()\n    runner = TestRunner(cm, lm)\n\n    all_llm_configs = cm.load_llms().get(\"llms\", [])\n    target_llms_configs = []\n    if args.llm == [\"all\"]:\n        target_llms_configs = all_llm_configs\n    else:\n        for llm_id_str in args.llm:\n            conf = cm.get_llm_config_by_id(llm_id_str)\n            if conf:\n                target_llms_configs.append(conf)\n            else:\n                print(f\"Warning: LLM ID '{llm_id_str}' not found in llms.json. Skipping.\")\n    \n    if not target_llms_configs:\n        print(\"No valid LLMs specified or found. Exiting.\")\n        return\n\n    all_tests_master_list = cm.load_tests().get(\"tests\", [])\n    tests_to_run_for_all_llms = []\n\n    if args.test == [\"all\"]:\n        tests_to_run_for_all_llms = all_tests_master_list\n    else:\n        for test_id_str in args.test:\n            instance = cm.get_test_instance_by_id(test_id_str)\n            if instance:\n                tests_to_run_for_all_llms.append(instance)\n            else:\n                print(f\"Warning: Test ID '{test_id_str}' not found in tests.json. Skipping.\")\n    \n    # Further filter by class and level if specified\n    if args.class_id:\n        tests_to_run_for_all_llms = [t for t in tests_to_run_for_all_llms if t['class_id'] in args.class_id]\n    if args.level is not None:\n        tests_to_run_for_all_llms = [t for t in tests_to_run_for_all_llms if t['level'] == args.level]\n\n    if not tests_to_run_for_all_llms:\n        print(\"No tests selected after filtering. Exiting.\")\n        return\n\n    print(f\"Target LLMs: {[lm.llm_id(c) for c in target_llms_configs]}\")\n    print(f\"Target Tests: {[t['test_id'] for t in tests_to_run_for_all_llms]}\")\n\n    tasks = []\n    for llm_c in target_llms_configs:\n        llm_id_str = lm.llm_id(llm_c)\n        specific_tests_for_this_llm = tests_to_run_for_all_llms\n        if not args.force:\n            pending_test_instances = cm.get_pending_tests(llm_id_str, [t['test_id'] for t in tests_to_run_for_all_llms])\n            pending_test_ids = {pt['test_id'] for pt in pending_test_instances}\n            specific_tests_for_this_llm = [t for t in tests_to_run_for_all_llms if t['test_id'] in pending_test_ids]\n            if not specific_tests_for_this_llm:\n                print(f\"No pending tests for LLM '{llm_id_str}' based on current selection. Use --force to re-run.\")\n                continue\n        \n        for test_instance in specific_tests_for_this_llm:\n            tasks.append((runner, test_instance['test_id'], llm_id_str))\n\n    if not tasks:\n        print(\"No tasks to run after considering pending tests and filters. Exiting.\")\n        return\n\n    print(f\"\\nStarting test execution for {len(tasks)} task(s)...\")\n    with ThreadPoolExecutor(max_workers=args.max_workers) as executor:\n        futures = {executor.submit(r.run_test, tid, lid): (tid, lid) for r, tid, lid in tasks}\n        for i, future in enumerate(as_completed(futures)):\n            test_id, llm_id = futures[future]\n            try:\n                result = future.result()\n                print(f\"({i+1}/{len(tasks)}) COMPLETED: Test '{test_id}' with LLM '{llm_id}'. Score: {result['score']:.2f}\")\n            except Exception as e:\n                print(f\"({i+1}/{len(tasks)}) FAILED: Test '{test_id}' with LLM '{llm_id}'. Error: {e}\")\n    \n    print(\"\\nAll selected tests completed.\")\n    # Optionally, run a summary report after tests\n    if args.auto_report:\n        print(\"\\nGenerating post-run summary report...\")\n        report_handler_args = argparse.Namespace(\n            config_dir=args.config_dir, format='summary', output=None, \n            llm=[lm.llm_id(c) for c in target_llms_configs],\n            test=[t['test_id'] for t in tests_to_run_for_all_llms]\n        )\n        report_handler(report_handler_args)\n\ndef report_handler(args):\n    base_path = args.config_dir if args.config_dir else None\n    cm = ConfigManager(base_path=base_path)\n    rg = ReportGenerator(cm)\n\n    llm_filter = args.llm if args.llm != [\"all\"] else None\n    test_filter = args.test if args.test != [\"all\"] else None\n\n    if args.format == 'summary':\n        rg.generate_summary_report(llm_ids=llm_filter, test_ids=test_filter)\n    elif args.format == 'csv':\n        if not args.output:\n            print(\"Error: --output filename is required for CSV format.\")\n            return\n        rg.generate_csv_report(args.output, llm_ids=llm_filter, test_ids=test_filter)\n    elif args.format == 'json':\n        if not args.output:\n            print(\"Error: --output filename is required for JSON format.\")\n            return\n        rg.generate_json_report(args.output, llm_ids=llm_filter, test_ids=test_filter)\n    else:\n        print(f\"Unknown report format: {args.format}\")\n\ndef list_handler(args):\n    base_path = args.config_dir if args.config_dir else None\n    cm = ConfigManager(base_path=base_path)\n    lm = LLMManager() # For consistent LLM ID generation\n\n    if args.item == 'llms':\n        llms_data = cm.load_llms().get(\"llms\", [])\n        if not llms_data: print(\"No LLMs configured.\")\n        for idx, llm_conf in enumerate(llms_data):\n            print(f\"- {lm.llm_id(llm_conf)} (Provider: {llm_conf['provider']}, Endpoint: {llm_conf['endpoint']})\")\n    elif args.item == 'tests':\n        tests_data = cm.load_tests().get(\"tests\", [])\n        if not tests_data: print(\"No tests configured.\")\n        for test in tests_data:\n            print(f\"- {test['test_id']} (Class: {test['class_id']}, Level: {test['level']})\")\n    elif args.item == 'classes':\n        classes_data = cm.load_test_classes().get(\"test_classes\", [])\n        if not classes_data: print(\"No test classes configured.\")\n        for tc in classes_data:\n            print(f\"- {tc['id']}: {tc['description']}\")\n    else:\n        print(f\"Unknown item to list: {args.item}. Choose from 'llms', 'tests', 'classes'.\")\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Poor Bench: A lightweight LLM benchmarking framework.\")\n    parser.add_argument('--config-dir', type=str, help=\"Path to the directory containing config files (test_classes.yaml, etc.). Defaults to 'poor_bench' directory where main.py is located.\")\n    subparsers = parser.add_subparsers(dest='command', required=True, help='Sub-command help')\n\n    # Run command\n    run_parser = subparsers.add_parser('run', help='Run tests')\n    run_parser.add_argument('--llm', type=str, nargs='+', default=['all'], help='LLM ID(s) (e.g., provider:name) or \"all\"')\n    run_parser.add_argument('--test', type=str, nargs='+', default=['all'], help='Test ID(s) or \"all\"')\n    run_parser.add_argument('--class-id', type=str, nargs='+', help='Filter tests by class ID(s)')\n    run_parser.add_argument('--level', type=int, help='Filter tests by difficulty level')\n    run_parser.add_argument('--force', action='store_true', help='Re-run already completed tests')\n    run_parser.add_argument('--max-workers', type=int, default=4, help='Maximum number of concurrent tests to run')\n    run_parser.add_argument('--auto-report', action='store_true', help='Generate a summary report after tests complete for the run selection')\n    run_parser.set_defaults(func=run_tests_handler)\n\n    # Report command\n    report_parser = subparsers.add_parser('report', help='Generate reports')\n    report_parser.add_argument('--format', type=str, choices=['summary', 'csv', 'json'], default='summary', help='Report format')\n    report_parser.add_argument('--output', type=str, help='Output filename (for CSV/JSON formats)')\n    report_parser.add_argument('--llm', type=str, nargs='+', default=['all'], help='Filter report by LLM ID(s) or \"all\"')\n    report_parser.add_argument('--test', type=str, nargs='+', default=['all'], help='Filter report by Test ID(s) or \"all\"')\n    report_parser.set_defaults(func=report_handler)\n\n    # List command\n    list_parser = subparsers.add_parser('list', help='List available configurations')\n    list_parser.add_argument('item', type=str, choices=['llms', 'tests', 'classes'], help='Item to list')\n    list_parser.set_defaults(func=list_handler)\n\n    args = parser.parse_args()\n    args.func(args)\n\nif __name__ == \"__main__\":\n    main()\n"
  }
]
```
